<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Boosting算法——AdaBoost（1）]]></title>
      <url>/2017/07/16/BoostAlgorithm-AdaBoost/</url>
      <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Boosting算法是一种将弱学习器提升为强学习器的算法，弱学习器指性能比随机猜测好一点的算法（层数不深的CAST算法就是一个好的选择）。Boosting算法的工作机制都是：先根据初始训练集训练出一个弱分类器（或者基本分类器），再根据这个弱分类器的表现对训练样本进行调整，使得之前弱分类器做错的的训练样本在后续训练中获取更多的关注，从而根据调整后的分布得到新的弱分类器。如此反复学习，得到一系列的弱分类器，然后组合这些弱分类器，得到一个强学习器。<br>比如有一组Data，它训练得到一个弱学习器$G(x)$<br><span>$$\begin{equation}
 \mathcal{D} = \{(x_1,y_1), (x_2,y_2),(x_3,y_3),(x_4,y_4),\}  
\end{equation}$$</span><!-- Has MathJax --><br>假设$\mathcal{D_{n}}$的分类误差为$\epsilon_m$,那么<br> <span>$$\begin{equation}
\epsilon_1 = P(G(x_i) \neq y_i) = \sum_{i=1}^{4}\frac{1}{4} I(G(x_i) \neq y_i) 
\end{equation}$$</span><!-- Has MathJax --><br>其中$I(x)$表示指示函数，即<br><span>$$\begin{equation}
I_{A} = \left\{
\begin{aligned}
&amp;1, x \in A \\
&amp;0, x  \not\in A
\end{aligned}
\right.
\end{equation}$$</span><!-- Has MathJax --><br>对于$\mathcal{D}$的初始弱分类器对与$(x_2,y_2)$分类出现错误，我们可以对这笔错误的$(x_2,y_2)$进行放大，比如重新调整$\mathcal{D}$的分布，得到<br><span>$$\begin{equation}
\mathcal{D_{1}} = \{(x_1,y_1), (x_2,y_2),(x_2,y_2),(x_4,y_4),\}
\end{equation}$$</span><!-- Has MathJax --><br>对于重新生成的分不$\mathcal{D_1}$,可以引入权重$w$来体现每笔Data的重要性，那么<br><span>$$\begin{aligned}
\epsilon_i= &amp;\sum_{i=1}^{4} w_i^{(m)} I(G(x_i) \neq y_i)  \\
&amp; (x_1, y_1), w_1 = \frac{1}{4}\\
&amp; (x_2, y_2), w_2 = \frac{2}{4}\\
&amp; (x_3, y_3), w_3 = 0\\
&amp; (x_4, y_4), w_4 = \frac{1}{4}
\end{aligned}$$</span><!-- Has MathJax --><br>这样也可以保证每次训练出来的若非类器不同。<br>常见的Boosting算法包括:AdaBoost， Gradient Boost 和 xgboost，希望可以通过三篇博客分别总结一下这三种算法。</p>
<h1 id="AdaBoost算法"><a href="#AdaBoost算法" class="headerlink" title="AdaBoost算法"></a>AdaBoost算法</h1><p>上面举了简单的例子， 现在拓展到一般的情况来得到AdaBoost算法。还是假设现有一笔Training Data，<br><span>$$\begin{equation}
\mathcal{D} = \{(x_1,y_1), (x_2,y_2),...,(x_N,y_N)\} 
\end{equation}$$</span><!-- Has MathJax --><br>其中$y_i \in \{-1, +1\}$.<br><span>$$\begin{equation}
G(\boldsymbol{x}) = \sum_{i=1}^{M} \alpha_i g_i(x)
\end{equation}$$</span><!-- Has MathJax --><br>其中$\alpha_i$表是每个弱分类器的重要性。<br>我们的目标是最小化分类error，利用指数损失函数<br><span>$$\begin{aligned}
L(G|\mathcal{D}) &amp;= E_{x\sim D}(e^{-\boldsymbol{y}G(\boldsymbol{x})})\\
&amp;=P(y=1|x)e^{-\boldsymbol{y}G(\boldsymbol{x})} + P(y=-1|x)e^{\boldsymbol{y}G(\boldsymbol{x})} 
\end{aligned}$$</span><!-- Has MathJax --><br>其中$E_{x\sim D}$表示$\boldsymbol{x}$在$\mathcal{D}$分布下的期望。<br>要使损失函数$L(G|\mathcal{D})$最小化,很自然的是对$G(x)$求偏导<br><span>$$\begin{equation}
\frac{\partial{L(G|\mathcal{D})}}{\partial{G(\boldsymbol(x))}} = -P(y=1|x)e^{-\boldsymbol{y}G(\boldsymbol{x})} + P(y=-1|x)e^{\boldsymbol{y}G(\boldsymbol{x})}
\end{equation}$$</span><!-- Has MathJax --><br>令导数等于0，可得<br><span>$$\begin{equation} 
G(x) =\frac{1}{2} \ln{\frac{P(y=1|x)}{P(y=-1|x)}} 
\end{equation}$$</span><!-- Has MathJax --><br>所以最后的分类器<br><span>$$\begin{aligned}
I(G(x)) &amp;= I(\frac{1}{2} \ln{\frac{P(y=1|x)}{P(y=-1|x)}})\\
&amp;= \left\{ 
    \begin{aligned}
    &amp; 1,  P(y=1|x)&gt;P(y=-1|x)\\
    &amp; 0,  P(y=1|x)&lt;P(y=-1|x)
    \end{aligned}
\right.\\
&amp;= \arg \max_{y\in\{-1,1\}}P(f(x)=y|x)
\end{aligned}$$</span><!-- Has MathJax --></p>
<p>所以指数损失函数是0/1损失函数的一致替代损失函数。并且指数函数是可微的，可以用它来替代0/1损失函数作为优化目标。<br>在Adaboost中，第一个弱分类器是由训练数据权值均匀分布$\mathcal{D_1}$的得到的，此后迭代获得弱分类器$g_i(x)$和重要性$\alpha_i$，且调整权值分布得到新的$\mathcal{D_i}$，当第$i$次迭代产生的对应于重要性$\alpha_i$的弱分类器$g_i(x)$，基于$\mathcal{D_i}$的损失函数为<br><span>$$\begin{aligned}
L(\alpha_i g_i|\mathcal{D_i}) &amp;= E_{x\sim\mathcal{D_i}}[e^{-y\alpha_ig_i(x)}] \\
&amp;=P_{x\sim\mathcal{D_i}}(y=g_i(x))e^{-\alpha_i} + P(y\neq g_i(x))e^{\alpha_i} \\
&amp;=e^{-\alpha_i}(1-\epsilon_i) + e^{\alpha_i}\epsilon_i
\end{aligned}$$</span><!-- Has MathJax --><br>其中$\epsilon_i=P(y\neq g_i(x))$表示当前分布${D_i}$下的分类error。对损失函数求导可得</p>
<span>$$\begin{equation}
\frac{\partial{L(\alpha_i g_i|\mathcal{D_i})}}{\partial \alpha_i} = -e^{-\alpha_i}(1-\epsilon_i) + e^{\alpha_i} \epsilon_i
\end{equation}$$</span><!-- Has MathJax -->
<p>令导数等于0，可解得<br><span>$$\begin{equation}
\alpha_i=\frac{1}{2}\ln \left(\frac{1-\epsilon_i}{\epsilon_i}\right)
\end{equation}$$</span><!-- Has MathJax --><br>所得到的$\alpha_i$表示$g_i$在最终分类器的重要性。因为每一个弱分类器的性能比随机猜测要好一些，所以分类error会小于0，因此$\alpha_i&gt;0$.并且随着$\epsilon_i$的减小，$\alpha_i$会增大，所以分类error越小的弱分类器在最终分类器中的作用会越大。<br>AdaBoost在获得$G_{i-1}$的弱学习器之后，对样本分布进行调整，使得下一轮的弱分类器可以纠正$G_{i-1}$的一些错误。理想的$G_{i}$可以纠正所有$G_{i-1}$的错误，即最小化<br><span>$$\begin{equation}
L(G_{i-1}+g_i|\mathcal{D}) = E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}e^{-yg_i(x)}]
\end{equation}$$</span><!-- Has MathJax --><br>对上式使用$e^{-yg_i(x)}$的二阶泰勒展开式近似，且因为$y^2=g^2(x)=1$,可得<br><span>$$\begin{aligned}
L(\alpha_i g_i|\mathcal{D_i}) &amp;\simeq E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}(1-yg_{i}(x)+\frac{y^2f_i^2(x)}{2})]\\
&amp;=E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}(1-yg_{i}(x)+\frac{1}{2})]
\end{aligned}$$</span><!-- Has MathJax --><br>所以理想的弱分类器<br><span>$$\begin{aligned}
h_t&amp;=\arg\min_hL(G_{i-1}+g_i|\mathcal{D})\\
&amp;= \arg\min_h E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}(x)(1-yg_{i}(x)+\frac{1}{2})]\\
&amp;= \arg\max E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}yg_{i}(x)]\\
&amp;= \arg\max E_{x\sim\mathcal{D}}\left[\frac{e^{-yG_{i-1}(x)}}{E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}]}yg_{i}(x)\right]
\end{aligned}$$</span><!-- Has MathJax --><br>因为$E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}]$是常数，所以不改变函数整体的单调性，令$\mathcal{D_i}$表示一个新的分布，<br><span>$$\begin{equation}
\mathcal{D_i}(x)=\frac{\mathcal{D}(x)e^{-yG_{i-1}(x)}}{E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}]}
\end{equation}$$</span><!-- Has MathJax --><br>因此，$g_i(x)$可以在$\mathcal{D_i}$下表示<br><span>$$\begin{aligned}
h_t(x) &amp;= \arg\max E_{x\sim\mathcal{D}}\left[\frac{e^{-yG_{i-1}(x)}}{E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}]}yg_{i}(x)\right]\\
&amp;= \arg\max E_{x\sim\mathcal{D_t}}[yg_i(x)]
\end{aligned}$$</span><!-- Has MathJax --><br>因为$g(x),y\in\{-1, +1\}$,所以<br><span>$$\begin{equation}
yg(x)=1-2I(y\neq g_i(x))
\end{equation}$$</span><!-- Has MathJax --><br>所以理想的弱分类器$g_i(x)$<br><span>$$\begin{equation}
g_i(x) = \arg\min E_{x\sim\mathcal{D_t}}[I(y\neq g_i(x))]
\end{equation}$$</span><!-- Has MathJax --><br>由此可见， 理想的弱学习器应该在分布$\mathcal{D_i}$下最小化分类误差。<br>此外，考虑分布$\mathcal{D_i}$和$\mathcal{D}_{i+1}$的关系，根据上面的推导过程，可知<br><span>$$\begin{equation}
\mathcal{D_i}(x)=\frac{\mathcal{D}(x)e^{-yG_{i-1}(x)}}{E_{x\sim\mathcal{D}}[e^{-yG_{i-1}(x)}]}
\end{equation}$$</span><!-- Has MathJax --><br>这边的分布为了在算法中更好理解，可以换成它的每一个元素是之前提到的权重$w$，那么对于$w$的更新即<br><span>$$\begin{aligned}
w_n^{i+1} &amp;= \frac{w_n^{i}}{Z_i}\exp(-\alpha_iy_ng_i(x_n))\\
&amp;=\left\{
    \begin{aligned}
    &amp;\frac{w_n^{i}}{Z_i}e^{-\alpha_i}, g_i(x_n)=y_n\\
    &amp;\frac{w_n^{i}}{Z_i}e^{\alpha_i}, g_i(x_n)\neq y_n
    \end{aligned}
\right.
\end{aligned}$$</span><!-- Has MathJax --><br>其中$Z_i$表示规划化因子， 因为$\sum_{n=1}^{N}w_{n} =1$<br><span>$Z_i=\sum_{n=1}^{N}w_{n}^{i}\exp(-\alpha_iy_ng_i(x_n))$</span><!-- Has MathJax --><br>由上可知，被弱分类器$g_i(x)$分类错误的样本权值将会被放大，分类正确的样本权值被缩小。两者相比，分类错误的样本权值被放大了$e^{2\alpha_i} = \frac{\epsilon_i}{1-\epsilon_i}$倍。<br>综上所述，那么AdaBoost的算法表格为<br><strong>Adaboost算法</strong>：<br> <strong>1.</strong> 输入数据集合$\mathcal{D} = \{(x_1,y_1), (x_2,y_2),…,(x_N,y_N)\} $<br> <strong>2.</strong> 初始化权值分布$\mathcal{D_1}=(w_1^{1},…w_N^{1})$,其中$w_n^{1}=\frac{1}{N}, n=1,…N$<br> <strong>3.</strong> 对$i=1,…M$<br> (a).使用具有权值分布 $\mathcal{D_i}$的数据训练，得到弱分类器$g_i(x)$<br> (b).计算$g_i(x)$在训练数据上的分类误差$\epsilon_i=\sum_{n=1}^{N}w_n^iI(g_i(x_n\neq g_i(x_n)))$<br> (c).计算$g_i(x)$的重要洗系数$\alpha_i=\frac{1}{2}\ln{\frac{1-\epsilon_i}{\epsilon_i}}$<br> (d).更新数据集的权值分布<br> <span>$$\begin{aligned}
\mathcal{D_{i+1}}=(w_1^{i},...w_N^{i})\\
 w_n^{i+1} = \frac{w_n^{i}}{Z_i}\exp(-\alpha_iy_ng_i(x_n)) \\
Z_i=\sum_{n=1}^{N}w_{n}^{i}\exp(-\alpha_iy_ng_i(x_n))
\end{aligned}$$</span><!-- Has MathJax --><br>其中$Z_i$是规范化因子。<br><strong>4.</strong>构建所有弱分类器的线性组合，得到最终分类器<br><span>$G(x)=I[\sum_{i=1}^{M}\alpha_iG_i(x)]$</span><!-- Has MathJax --></p>
<h1 id="编程仿真"><a href="#编程仿真" class="headerlink" title="编程仿真"></a>编程仿真</h1><p>首先产生一组数据，我这里产生分割线为sin函数的一组数据<br>数据产生代码：</p>
<pre class=" language-lang-python"><code class="language-lang-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def GenerateSinData(length):
    x = 2 * np.pi * np.random.random(length)
    y = 4 * np.random.random(length)
    i = 0
    result = np.zeros(x.shape)
    while i < length:
        if(y[i] >= np.sin(x[i]) + 2):
            result[i] = 1
            plt.plot(x[i], y[i], 'ro')
        else:
            result[i] = -1
            plt.plot(x[i], y[i], 'bx')
        i = i+1
    plt.show()
    X = np.array([x, y]).T
    return X , result
</code></pre>
<p><img src="\img\TrainningData.png" alt="TrainningData"><br>弱学习我选择了DecisionStrump，单层CART树，如果将它直接应用于对上面数据的分类，由于是单层树，只能产生横线或者竖线，分类效果并不理想。<br>DecisionStrump代码：</p>
<pre class=" language-lang-python"><code class="language-lang-python">import numpy as np
def StumpClassify(X, j, thresh, ineq):
    '''
    X:数据集       j:属性列
    thresh:阈值       ineq:比较
    '''
    pred = np.ones(X.shape[0])
    if ineq == 'lt':
        pred[X[:, j] <= thresh] = -1
    else:
        pred[X[:, j] > thresh] = 1
    return pred

def buildStump(X, y, w):
    '''
    X: 数据集
    y: labels
    w: 权值, 初始化为 np.ones(N)/N
    '''
    N, d = X.shape
    #w = np.ones(N)/N
    minErr = np.inf
    numStep = 20
    bestStump, bestLabel = {}, np.zeros(N)

    for j in range(d):
        rangeMin = X[:, j].min() 
        rangeMax = X[:, j].max()
        stepSize = (rangeMax - rangeMin)/numStep
        for i in range(0, numStep+1):
            thresh = rangeMin + i * stepSize
            for ineq in ['lt', 'gt']:
                pred = StumpClassify(X, j, thresh, ineq)
                errLabel = np.ones(N)
                errLabel[pred == y] = 0
                weightedErr = w.dot(errLabel)
                if minErr > weightedErr:
                    minErr = weightedErr
                    bestClass = pred
                    bestStump['dim'] = j
                    bestStump['ineq'] = ineq
                    bestStump['thresh'] = thresh
    return bestStump, minErr, bestClass
</code></pre>
<p>最后得到的分类误差是0.125<br><img src="\img\DecisionStumpResult.png" alt="TrainningData"></p>
<p>将DecisionStrump作为弱学习器应用于AdaBoost,进行10次迭代，可以看出分类误差得到了明显的下降</p>
<pre class=" language-lang-python"><code class="language-lang-python">import numpy as np
from DecisionStump import *

def AdaboostTest(DataArr, classLabel, iteration):
    weakClassArr = []
    N = DataArr.shape[0]
    D = np.ones(N)/N
    aggClassEst = np.zeros(N)
    for i in range(iteration):
        bestStump, error, classEst = buildStump(DataArr, classLabel, D)

        alpha = float(0.5*np.log((1-error)/max(error, 1e-16)))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        expon = np.multiply(-1*alpha*classLabel.T, classEst)
        D = np.multiply(D, np.exp(expon))
        D = D/D.sum()
        aggClassEst += alpha*classEst

        aggErrors = np.multiply(np.sign(aggClassEst) != classLabel.T, np.ones(N))
        errorRate = aggErrors.sum()/N
        print("Total errtor", errorRate, "\n")
        if errorRate == 0.0: 
            break
    return  weakClassArr
</code></pre>
<p>Total error 0.125<br>Total error 0.125<br>Total error 0.085<br>Total error 0.06<br>Total error 0.06<br>Total error 0.06<br>Total error 0.04<br>Total error 0.04<br>Total error 0.04<br>Total error 0.045<br><img src="\img\AdaBoostResult.png" alt="TrainningData"></p>
<h1 id="尾巴"><a href="#尾巴" class="headerlink" title="尾巴"></a>尾巴</h1><p>这是我第一次写博客，选择AdaBoost算法开始是因为前段时间正好看完了台大林轩田老师关于Blending的内容，觉得由推导到算法都很神奇。这篇博客内容虽然并不是很多，但是我写了大概一天（一直在打公式），感觉条理不明确，思路也不清晰，大多都是在参考书上的内容，自己的理解比较少，希望自己可以继续加油，在以后的博客里多增加一些自己的思考。</p>
<p><strong>参考：</strong><br>1、统计学习方法—李航<br>2、机器学习—周志华<br>3、机器学习实战<br>4、机器学习技法—台湾大学林轩田老师公开课</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Boosting </tag>
            
            <tag> AdaBoost </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[第一篇博客]]></title>
      <url>/2017/07/12/first_blog/</url>
      <content type="html"><![CDATA[<p>这位是我的第一篇博客，我将做一些测试</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"
</code></pre>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="代码测试"><a href="#代码测试" class="headerlink" title="代码测试"></a>代码测试</h3><pre class=" language-lang-python"><code class="language-lang-python">@requires_authorization
class SomeClass:
    pass

if __name__ == '__main__':
    # A comment
    print 'hello world'
</code></pre>
<pre class=" language-lang-java"><code class="language-lang-java">import java.util.ArrayList
public class Test{
    public void blotTest(String s){
        System.out.println("This is my first blog")
    }
}
</code></pre>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="公式测试"><a href="#公式测试" class="headerlink" title="公式测试"></a>公式测试</h3><span>$$\begin{aligned}
\dot{x} &amp; = \sigma(y-x) \\
\dot{y} &amp; = \rho x - y - xz \\
\dot{z} &amp; = -\beta z + xy
\end{aligned}$$</span><!-- Has MathJax -->
<script type="math/tex; mode=display">\frac{\partial u}{\partial t}
= h^2 \left( \frac{\partial^2 u}{\partial x^2} +
\frac{\partial^2 u}{\partial y^2} +
\frac{\partial^2 u}{\partial z^2}\right)</script><p>Simple inline $a = b + c$.</p>
<p>\begin{equation}\notag<br>    \boldsymbol{y} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{n}<br>\end{equation}</p>
<script type="math/tex; mode=display">E=mc^2</script><p>\begin{equation}<br>y=Ax+n<br>\end{equation}</p>
<script type="math/tex; mode=display">\sum_{i=1}^n a_i=0</script><p>\begin{align}<br>    \boldsymbol{y} &amp;= \boldsymbol{A} \boldsymbol{x} + \boldsymbol{n}\\<br>    &amp; = \boldsymbol{\phi} \boldsymbol{\theta} \boldsymbol{x} + \boldsymbol{n}<br>\end{align}</p>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy
</code></pre>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
      
        <categories>
            
            <category> 生活琐事 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 琐碎随笔, 测试 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
